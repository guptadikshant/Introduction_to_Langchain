{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1badc36-65c4-4233-8338-c86662336a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38c219a-8c6c-40bf-b649-518759548b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28.0\n"
     ]
    }
   ],
   "source": [
    "print(openai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d34d4de7-9464-4195-b13c-43d41ea47067",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88d34f4-3019-4c68-8da8-af56d1072e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37249085-bc7c-4a3f-a029-506bafaba69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion=openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a assistant which informs about temperature.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hey there\"}\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c1675ed-94ec-4b59-b5f7-a40f307d103c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-96Mg2Wy0hJmSwbb4TW9zsFLKaaRUb at 0x2a7b6863dd0> JSON: {\n",
       "  \"id\": \"chatcmpl-96Mg2Wy0hJmSwbb4TW9zsFLKaaRUb\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1711304166,\n",
       "  \"model\": \"gpt-3.5-turbo-0125\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Hello! How can I assist you today?\"\n",
       "      },\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 22,\n",
       "    \"completion_tokens\": 9,\n",
       "    \"total_tokens\": 31\n",
       "  },\n",
       "  \"system_fingerprint\": \"fp_3bc1b5746c\"\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8237da",
   "metadata": {},
   "source": [
    "## Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fc4ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3204d661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\PlayGround\\GEN_AI\\Introduction_to_Langchain\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Instatiate your language model\n",
    "chat = ChatOpenAI(temperature=0, \n",
    "                  api_key=OPENAI_API_KEY,\n",
    "      \n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4c990",
   "metadata": {},
   "source": [
    "### 1) Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e789156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will be more often than not be work with complex strings but for now lets start with a simple one!\n",
    "my_text = \"Explain a Large Language Model in one line?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c59a2864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\PlayGround\\GEN_AI\\Introduction_to_Langchain\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting response for the question from the language model\n",
    "chat.predict(\"hello there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f45bf0",
   "metadata": {},
   "source": [
    "### 2) Chat Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c20c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f6af8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\PlayGround\\GEN_AI\\Introduction_to_Langchain\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='How about a delicious turkey and avocado sandwich? Simply layer sliced turkey, creamy avocado, crisp lettuce, and your favorite condiments between two slices of toasted bread for a satisfying and flavorful meal. Enjoy!', response_metadata={'token_usage': <OpenAIObject at 0x2a7b8dfc710> JSON: {\n",
       "  \"prompt_tokens\": 46,\n",
       "  \"completion_tokens\": 40,\n",
       "  \"total_tokens\": 86\n",
       "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are an internation chef that helps who specializes in making sandwiches\"),\n",
    "        HumanMessage(content=\"I dont like tomatoes, what else can make me a sandwhich? Give a 2 line recipie.\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0050be6",
   "metadata": {},
   "source": [
    "### 3) Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aff49734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "564386c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='This is the private/custom text/unstructured data. Can also be imported from multiple sources', metadata={'my_document_id': 908475, 'my_document_source': 'Some hypothetical source'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"\"\"This is the private/custom text/unstructured data. Can also be imported from multiple sources\"\"\",\n",
    "         metadata={\n",
    "             'my_document_id' : 908475,\n",
    "             'my_document_source' : \"Some hypothetical source\"\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ec0c93",
   "metadata": {},
   "source": [
    "## Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03fa609",
   "metadata": {},
   "source": [
    "### a) Simple Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e4e2b8f-a1e0-44ec-9a6f-f63383034422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Model model id=gpt-3.5-turbo-1106 at 0x2a7b9922750> JSON: {\n",
       "   \"id\": \"gpt-3.5-turbo-1106\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1698959748,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=dall-e-3 at 0x2a7b99224b0> JSON: {\n",
       "   \"id\": \"dall-e-3\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1698785189,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=gpt-4-0613 at 0x2a7b98f3e30> JSON: {\n",
       "   \"id\": \"gpt-4-0613\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1686588896,\n",
       "   \"owned_by\": \"openai\"\n",
       " },\n",
       " <Model model id=dall-e-2 at 0x2a7b98f3fb0> JSON: {\n",
       "   \"id\": \"dall-e-2\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1698798177,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=gpt-4 at 0x2a7b98f3e90> JSON: {\n",
       "   \"id\": \"gpt-4\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1687882411,\n",
       "   \"owned_by\": \"openai\"\n",
       " },\n",
       " <Model model id=whisper-1 at 0x2a7b98f01d0> JSON: {\n",
       "   \"id\": \"whisper-1\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1677532384,\n",
       "   \"owned_by\": \"openai-internal\"\n",
       " },\n",
       " <Model model id=tts-1-hd-1106 at 0x2a7b98f06b0> JSON: {\n",
       "   \"id\": \"tts-1-hd-1106\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1699053533,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=tts-1-hd at 0x2a7b98f0770> JSON: {\n",
       "   \"id\": \"tts-1-hd\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1699046015,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=text-embedding-3-large at 0x2a7b98f0b30> JSON: {\n",
       "   \"id\": \"text-embedding-3-large\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1705953180,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=gpt-3.5-turbo-0301 at 0x2a7b98f0bf0> JSON: {\n",
       "   \"id\": \"gpt-3.5-turbo-0301\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1677649963,\n",
       "   \"owned_by\": \"openai\"\n",
       " },\n",
       " <Model model id=gpt-3.5-turbo-0613 at 0x2a7b98f0ef0> JSON: {\n",
       "   \"id\": \"gpt-3.5-turbo-0613\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1686587434,\n",
       "   \"owned_by\": \"openai\"\n",
       " },\n",
       " <Model model id=gpt-3.5-turbo at 0x2a7b98f0f50> JSON: {\n",
       "   \"id\": \"gpt-3.5-turbo\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1677610602,\n",
       "   \"owned_by\": \"openai\"\n",
       " },\n",
       " <Model model id=gpt-4-vision-preview at 0x2a7b98f1250> JSON: {\n",
       "   \"id\": \"gpt-4-vision-preview\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1698894917,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=gpt-3.5-turbo-16k-0613 at 0x2a7b98f1550> JSON: {\n",
       "   \"id\": \"gpt-3.5-turbo-16k-0613\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1685474247,\n",
       "   \"owned_by\": \"openai\"\n",
       " },\n",
       " <Model model id=gpt-3.5-turbo-0125 at 0x2a7b98f14f0> JSON: {\n",
       "   \"id\": \"gpt-3.5-turbo-0125\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1706048358,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=gpt-3.5-turbo-16k at 0x2a7b98f15b0> JSON: {\n",
       "   \"id\": \"gpt-3.5-turbo-16k\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1683758102,\n",
       "   \"owned_by\": \"openai-internal\"\n",
       " },\n",
       " <Model model id=gpt-4-1106-preview at 0x2a7b98f0fb0> JSON: {\n",
       "   \"id\": \"gpt-4-1106-preview\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1698957206,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=gpt-3.5-turbo-instruct at 0x2a7b98f1370> JSON: {\n",
       "   \"id\": \"gpt-3.5-turbo-instruct\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1692901427,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=gpt-3.5-turbo-instruct-0914 at 0x2a7b98f1a30> JSON: {\n",
       "   \"id\": \"gpt-3.5-turbo-instruct-0914\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1694122472,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=tts-1 at 0x2a7b98f1af0> JSON: {\n",
       "   \"id\": \"tts-1\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1681940951,\n",
       "   \"owned_by\": \"openai-internal\"\n",
       " },\n",
       " <Model model id=davinci-002 at 0x2a7b98f1fd0> JSON: {\n",
       "   \"id\": \"davinci-002\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1692634301,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=babbage-002 at 0x2a7b98f2030> JSON: {\n",
       "   \"id\": \"babbage-002\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1692634615,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=tts-1-1106 at 0x2a7b98f2390> JSON: {\n",
       "   \"id\": \"tts-1-1106\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1699053241,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=gpt-4-0125-preview at 0x2a7b98f23f0> JSON: {\n",
       "   \"id\": \"gpt-4-0125-preview\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1706037612,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=gpt-4-turbo-preview at 0x2a7b98f26f0> JSON: {\n",
       "   \"id\": \"gpt-4-turbo-preview\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1706037777,\n",
       "   \"owned_by\": \"system\"\n",
       " },\n",
       " <Model model id=text-embedding-ada-002 at 0x2a7b98f29f0> JSON: {\n",
       "   \"id\": \"text-embedding-ada-002\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1671217299,\n",
       "   \"owned_by\": \"openai-internal\"\n",
       " },\n",
       " <Model model id=text-embedding-3-small at 0x2a7b98f2990> JSON: {\n",
       "   \"id\": \"text-embedding-3-small\",\n",
       "   \"object\": \"model\",\n",
       "   \"created\": 1705948997,\n",
       "   \"owned_by\": \"system\"\n",
       " }]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.Model.list()[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14e3d3f5-d1f6-42aa-921c-dd5cc8a35f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6e8cbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\PlayGround\\GEN_AI\\Introduction_to_Langchain\\venv\\Lib\\site-packages\\langchain_community\\llms\\openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "F:\\PlayGround\\GEN_AI\\Introduction_to_Langchain\\venv\\Lib\\site-packages\\langchain_community\\llms\\openai.py:1070: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-0125\", api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d485fcda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saturday'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What day comes after Friday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf8ed3",
   "metadata": {},
   "source": [
    "### b) Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf19252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1, api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbce49f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Oh, you simply close your eyes and hope for the best! Just kidding, please don't do that. Try asking a real map for directions, they might be more helpful than me!\", response_metadata={'token_usage': <OpenAIObject at 0x2a7b9b96bd0> JSON: {\n",
       "  \"prompt_tokens\": 36,\n",
       "  \"completion_tokens\": 38,\n",
       "  \"total_tokens\": 74\n",
       "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a dumb AI bot that is unhelpfull and makes jokes at whatever the user says\"),\n",
    "        HumanMessage(content=\"how do I navigate maps?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9078222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To navigate maps, simply enter your destination in the search bar, then follow the suggested route displayed on the map. You can zoom in or out for a better view and utilize the turn-by-turn directions for guidance.', response_metadata={'token_usage': <OpenAIObject at 0x2a7b9b97770> JSON: {\n",
       "  \"prompt_tokens\": 42,\n",
       "  \"completion_tokens\": 43,\n",
       "  \"total_tokens\": 85\n",
       "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are an intelligent AI bot that tries to help the the user in every way possible.\"),\n",
    "        HumanMessage(content=\"how do I navigate maps? explain in 2-3 lines\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ecbe89",
   "metadata": {},
   "source": [
    "### c) Text Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e933d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Instantiate the embedding model you want to use. \n",
    "# Here we are using OpenAI's Embeddings which returns a vector representation that is 1536 in length\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2585822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dummy query\n",
    "text = \"How do I navigate maps? explain in 2-3 lines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1a4f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the embedidngs\n",
    "text_embedding = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbf70a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your embedding is length 1536\n",
      "Here's a sample: [0.0030461493879556656, 0.005973175633698702, 0.015479203313589096, 0.001639645197428763, -0.02270827814936638]...\n"
     ]
    }
   ],
   "source": [
    "# Print the length of your embedding\n",
    "print (f\"Your embedding is length {len(text_embedding)}\")\n",
    "\n",
    "# Print first 5 numerical representations from your embeding\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7477e",
   "metadata": {},
   "source": [
    "## Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "053b2939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# This will help us build the templates\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da4ab86",
   "metadata": {},
   "source": [
    "### a) Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e571d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice \"career_option\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "I want to be {career_option} in future. What subjects should I start studying?\n",
    "\n",
    "Respond in 1-2 short sentence\n",
    "\"\"\"\n",
    "\n",
    "# Creating a template from the above prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"career_option\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b7487d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "I want to be Data Scientist in future. What subjects should I start studying?\n",
      "\n",
      "Respond in 1-2 short sentence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Passing a value to the placeholder\n",
    "final_prompt = prompt.format(career_option='Data Scientist')\n",
    "\n",
    "# Print the final prompt with placeholder value\n",
    "print (f\"Final Prompt: {final_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9bcb05ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Output: \n",
      "To become a Data Scientist, you should start studying mathematics, statistics, computer science, and programming. Additionally, you should also focus on developing your analytical and problem-solving skills.\n"
     ]
    }
   ],
   "source": [
    "# Pass it to the llm to get the output\n",
    "print (f\"LLM Output: {llm(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24aec73",
   "metadata": {},
   "source": [
    "### b) Example Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7396320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of job roles and respective job titles\n",
    "examples = [\n",
    "    {\"input\": \"software engineer\", \"output\": \"software development\"},\n",
    "    {\"input\": \"accountant\", \"output\": \"accounting\"},\n",
    "    {\"input\": \"teacher\", \"output\": \"education\"},\n",
    "    {\"input\": \"doctor\", \"output\": \"medicine\"},\n",
    "    {\"input\": \"architect\", \"output\": \"architecture\"},\n",
    "    {\"input\": \"lawyer\", \"output\": \"law\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b74803d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
    "\n",
    "selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples, \n",
    "    \n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY), \n",
    "    \n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS, \n",
    "    \n",
    "    # This is the number of examples to produce.\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "285a0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples\n",
    "    example_selector=selector,\n",
    "    \n",
    "    # Your prompt\n",
    "    example_prompt=prompt,\n",
    "    \n",
    "    # Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the job title their job role is \",\n",
    "    suffix=\"Input: {job_title}\\nOutput:\",\n",
    "    \n",
    "    # What inputs your prompt will receive\n",
    "    input_variables=[\"job_title\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15900c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the job title their job role is \n",
      "\n",
      "Example Input: doctor\n",
      "Example Output: medicine\n",
      "\n",
      "Example Input: lawyer\n",
      "Example Output: law\n",
      "\n",
      "Input: nurse\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Select a Job Title!\n",
    "title = \"nurse\"\n",
    "\n",
    "# Print the 2 closest examples to the input, this selection is \n",
    "# done by the \"SemanticSimilarityExampleSelector\"\n",
    "print(similar_prompt.format(job_title=title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b9120f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' healthcare'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based on the  selected eg. ask the LLM to give output.\n",
    "llm(similar_prompt.format(job_title=title))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc454c",
   "metadata": {},
   "source": [
    "## Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb117741",
   "metadata": {},
   "source": [
    "### a) Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48213ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PDF document loaders...there are lot of other ones as well!\n",
    "from langchain.document_loaders. import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "340f37bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load PDF Loader\n",
    "loader = PyPDFLoader('annualreport.pdf')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37e31378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 312 comments\n"
     ]
    }
   ],
   "source": [
    "# Printing total number of documents in the loader object\n",
    "print (f\"Found {len(data)} comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27ec4688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Annual  \\nReport\\n2022\\nMacquarie Group Limited\\nACN 122 169 279Macquarie Group\\nYear ended 31 March 2022', metadata={'source': 'annualreport.pdf', 'page': 0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the 1st document, observe there is \"page_content\" and \"metadata\" associated with it!\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b0335369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following is data of first 2 documents:\n",
      "\n",
      "Annual  \n",
      "Report\n",
      "2022\n",
      "Macquarie Group Limited\n",
      "ACN 122 169 279Macquarie Group\n",
      "Year ended 31 March 20222022 Annual General Meeting\n",
      "Macquarie Group Limited’s 2022 AGM will be held at 10:30 am on \n",
      "Thursday, 28 July 2022.\n",
      "Details of the meeting will be sen\n"
     ]
    }
   ],
   "source": [
    "# Extracting only the \"page_content\" of the first 2 documents\n",
    "print (f\"Following is data of first 2 documents:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b379a",
   "metadata": {},
   "source": [
    "### b) Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c152758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary Text splitter \n",
    "from langchain.text_splitter import PythonCodeTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2848b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Splitter\n",
    "text_splitter = PythonCodeTextSplitter(chunk_size = 1000, chunk_overlap=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7394aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split pages from pdf using the object\n",
    "pages = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f2c7ee94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1399 documents\n"
     ]
    }
   ],
   "source": [
    "# Remember we originally had 312 documents, lets check how many we have after chunking\n",
    "print (f\"You have {len(pages)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ff9fa94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual  \n",
      "Report\n",
      "2022\n",
      "Macquarie Group Limited\n",
      "ACN 122 169 279Macquarie Group\n",
      "Year ended 31 March 2022 \n",
      "\n",
      "2022 Annual General Meeting\n",
      "Macquarie Group Limited’s 2022 AGM will be held at 10:30 am on \n",
      "Thursday, 28 July 2022.\n",
      "Details of the meeting will be sent to shareholders separately.\n",
      "Cover image\n",
      "Macquarie manages investments in infrastructure integral to \n",
      "the US economy. Long Beach Container Terminal is part of the \n",
      "combined port complex of Los Angeles and Long Beach, the \n",
      "largest cargo gateway in North America.\n",
      "Macquarie is a global financial \n",
      "services group operating in \n",
      "33 markets in asset management, \n",
      "retail and business banking, wealth \n",
      "management, leasing and asset \n",
      "financing, market access, commodity \n",
      "trading, renewables development, \n",
      "specialist advice, access to capital \n",
      "and principal investment.\n"
     ]
    }
   ],
   "source": [
    "print (pages[0].page_content, \"\\n\")\n",
    "print (pages[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa7ec6b",
   "metadata": {},
   "source": [
    "## Vector Stores & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7cbb58cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Chroma as the vector store\n",
    "# ChromaDB is an in-memory vector store\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "04da2ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents into vector database\n",
    "store_chroma = Chroma.from_documents(documents=pages, \n",
    "                                     embedding=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY),\n",
    "                                     collection_name=\"annual_report\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bd2e6701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2022 Annual General Meeting\\nMacquarie Group Limited’s 2022 AGM will be held at 10:30 am on \\nThursday, 28 July 2022.\\nDetails of the meeting will be sent to shareholders separately.\\nCover image\\nMacquarie manages investments in infrastructure integral to \\nthe US economy. Long Beach Container Terminal is part of the \\ncombined port complex of Los Angeles and Long Beach, the \\nlargest cargo gateway in North America.\\nMacquarie is a global financial \\nservices group operating in \\n33\\xa0markets in asset management, \\nretail and business banking, wealth \\nmanagement, leasing and asset \\nfinancing, market access, commodity \\ntrading, renewables development, \\nspecialist advice, access to capital \\nand principal investment.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarites = store_chroma.similarity_search(\"When was Macquarie Group's AGM held?\")\n",
    "\n",
    "# Display no. of similar docs found\n",
    "print(len(similarites))\n",
    "\n",
    "# Display the doc having context most closet to the asked query.\n",
    "similarites[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e06c307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating Embedding model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Creating Embeddings\n",
    "embedding_list = embeddings.embed_documents([doc.page_content for doc in pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6bc91def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1399 embeddings\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(embedding_list)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c3b85374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a part of embedding of first doc: [-0.023503716178620307, -0.032320840969157806, -0.006800304850503839]...\n"
     ]
    }
   ],
   "source": [
    "print (f\"Here's a part of embedding of first doc: {embedding_list[0][:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d344d6e",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0f8d9efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "# Instantiating basic chat history object\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# Adding an AI message into it followed by specific question as user message\n",
    "history.add_ai_message(\"hi!\")\n",
    "history.add_user_message(\"Give me a 2 line recipie to make sandwich? By the way I dont like tomatoes in my sandwhich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e9dd88f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Give me a 2 line recipie to make sandwich? By the way I dont like tomatoes in my sandwhich', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see the messages stored in history by using '.messages' method\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "567d17bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Spread mayonnaise on two slices of bread, layer with your favorite deli meats and cheese, and enjoy your tomato-free sandwich!', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initializing a ChatBot model\n",
    "chat = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Getting the AI response by passing the messages in history as input\n",
    "ai_response = chat(history.messages)\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f097ed34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Italy is famous for pizza.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now asking a completely random out of context question and storing it in history as well\n",
    "history.add_user_message(\"Which country is famous for pizza?\")\n",
    "\n",
    "ai_response = chat(history.messages)\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b4d1b0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To make a sandwich without tomatoes, simply layer your favorite bread with your choice of deli meats, cheese, and any other desired toppings. Enjoy!', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then when we ask query based on 1st scenario, still by virtue of history \n",
    "# the model is able to remember that tomatoes are disliked\n",
    "history.add_user_message(\"Which ingridient do I not like in my sandwich?\")\n",
    "\n",
    "ai_response = chat(history.messages)\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b77d79",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b668d84",
   "metadata": {},
   "source": [
    "### a) Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b63bd8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = OpenAI(temperature=1, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7cd4fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template that take user input directly\n",
    "template = \"\"\"Your task is to find the movie name which won Academy Awards / Best Picture in the year that user suggests.\n",
    "% YEAR\n",
    "{year}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"year\"], template=template)\n",
    "\n",
    "# Holds the 'year' chain\n",
    "year_chain = LLMChain(llm=llm, prompt=prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6c2656a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template that will consume output of the previous LLM chain\n",
    "template = \"\"\"Given the movie name, give a short summaruy of the plot of that movie.\n",
    "% MOVIE_NAME\n",
    "{movie_name}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"movie_name\"], template=template)\n",
    "\n",
    "# Holds the 'plot' chain\n",
    "plot_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "18cc7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the Simple SEquential Chain, the order in which chains are passed matters here!!\n",
    "movie_plot_chain = SimpleSequentialChain(chains=[year_chain, plot_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bc803dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mThe 2020 Best Picture Academy Award was won by the movie Parasite.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mParasite is an award-winning South Korean black comedy that follows the struggling Kim family as they become involved in the lives of the wealthy Park family. Through a series of comical misfortunes, the Kims slowly take over the Parks' lives and find themselves in a precarious situation. With unexpected twists and turns that explore the deep divide between rich and poor, Parasite ultimately reveals the lengths the Kims will go to in order to gain wealth and security.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Executing the chain, observe that we just gave the year and nothing else, \n",
    "# still the chain found the movie name and also summarized its plot!! \n",
    "review = movie_plot_chain.run(\"2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599509f",
   "metadata": {},
   "source": [
    "### b) Summary Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "873a3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "67ab65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the summary chain\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3748d355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2022 Annual General Meeting\n",
      "Macquarie Group Limited’s 2022 AGM will be held at 10:30 am on \n",
      "Thursday, 28 July 2022.\n",
      "Details of the meeting will be sent to shareholders separately.\n",
      "Cover image\n",
      "Macquarie manages investments in infrastructure integral to \n",
      "the US economy. Long Beach Container Terminal is part of the \n",
      "combined port complex of Los Angeles and Long Beach, the \n",
      "largest cargo gateway in North America.\n",
      "Macquarie is a global financial \n",
      "services group operating in \n",
      "33 markets in asset management, \n",
      "retail and business banking, wealth \n",
      "management, leasing and asset \n",
      "financing, market access, commodity \n",
      "trading, renewables development, \n",
      "specialist advice, access to capital \n",
      "and principal investment.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Contents\n",
      "03 04\n",
      "05Directors’ Report\n",
      "Directors’ Report  83\n",
      "Directors’ experience and \n",
      "special responsibilities  88\n",
      "Remuneration Report  94Financial Report\n",
      "Financial statements\n",
      "Income statements  147\n",
      "Statements of comprehensive income  148\n",
      "Statements of financial position  149\n",
      "Statements of changes in equity  150\n",
      "Statements of cash flows  152\n",
      "Notes to the financial statements  153\n",
      "Statutory statements\n",
      "Directors’ declaration  288\n",
      "Independent auditor’s report  289\n",
      "Further Information\n",
      "Additional investor information  298\n",
      "Ten Year History  304\n",
      "Glossary  30501 02\n",
      "About\n",
      "About Macquarie 5\n",
      "Letter from the Chairman 6\n",
      "Letter from the Managing Director  \n",
      "and CEO 8\n",
      "Empowering people to innovate  \n",
      "and invest for a better future 10\n",
      "Financial Highlights  14\n",
      "Operating and Financial Review 26Governance\n",
      "Corporate Governance 32\n",
      "Diversity, Equity and Inclusion 40\n",
      "Environmental, Social and Governance 46\n",
      "Macquarie Group Foundation 68\n",
      "Risk Management  72\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2\n",
      "01\n",
      "About\n",
      "Sotra Link, Norway\n",
      "Macquarie Capital is the lead sponsor and equity \n",
      "investor in Sotra Link, an extensive new state-of-the -art \n",
      "bridge and tunnel system in Norway that focuses on \n",
      "traffic safety, public transport, and sustainability.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" Macquarie Group Limited will host their 2022 Annual General Meeting on 28th July 2022 at 10:30am. Additional details will be sent to shareholders closers to the event. Macquarie is a financial services group operating in 33 markets, specializing in managing and financing infrastructure integral to the US Economy.\n",
      "\n",
      " The document includes a Directors’ Report, Remuneration Report, Financial Report, Statutory Statements, Further Information, Additional Investor Information, Ten Year History, Glossary, and information About Macquarie. It provides details on corporate governance, diversity, equity and inclusion, environmental, social and governance, risk management, financial statements, income statements, and more.\n",
      "\n",
      " Macquarie Capital is the main sponsor and equity investor in the Norwegian Sotra Link project, a cutting edge bridge and tunnel construction designed to improve transportation safety, public transit, and environmental sustainability.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Macquarie Group Limited will host their 2022 Annual General Meeting on 28th July 2022, providing details on corporate governance, diversity, equity and inclusion, environmental, social and governance, risk management, and more. Macquarie Capital is also the main sponsor and equity investor in the Norwegian Sotra Link project, a cutting edge bridge and tunnel construction designed to improve transportation safety, public transit, and environmental sustainability.'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarizing first 3 documnets from the \"Annual Report\" PDF we saw previously\n",
    "chain.run(pages[1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223e5907",
   "metadata": {},
   "source": [
    "## Tools & Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "95e479b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERPAPI_API_KEY = '12d80712d6c6183ba8817c6bb3b896c5de3c37277d7398e6bbb33a15f74d550a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7cf582c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to Load tools based on their name.\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "# class to load an agent executor given the tools and LLM.\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1fc45199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a toolkit (collection of tools), this example we only use \"serpapi\" (search engine) as a tool.\n",
    "SERPAPI_API_KEY = '<ENTER_YOUR_API_KEY>'\n",
    "\n",
    "toolkit = load_tools(tool_names=[\"serpapi\"],  # there are plenty of tools, which can be used based on requirement\n",
    "                     llm=llm, \n",
    "                    serpapi_api_key=SERPAPI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0861687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an agent exutor, this will make decision depending on the scenario and facilitate output generation\n",
    "agent = initialize_agent(toolkit, \n",
    "                         llm, \n",
    "                         agent=\"zero-shot-react-description\",\n",
    "                         verbose=True, \n",
    "                         return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "619b38b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out which club Ronaldo played for in 2023 and when he left Real Madrid.\n",
      "Action: Search\n",
      "Action Input: \"Cristiano Ronaldo football club 2023\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAl-Nassr\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out when Ronaldo left Real Madrid\n",
      "Action: Search\n",
      "Action Input: \"Cristiano Ronaldo Real Madrid\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mCristiano Ronaldo dos Santos Aveiro GOIH ComM is a Portuguese professional footballer who plays as a forward for and captains both Saudi Professional League club Al Nassr and the Portugal national team.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out how many years it has been since he left Real Madrid\n",
      "Action: Calculator\n",
      "Action Input: 2020 - 2018\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 2 years since Cristiano Ronaldo left Real Madrid in 2018 to join Al-Nassr in 2023.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_text = '''Print for which football club did Cristiano Ronaldo play in 2023? and \n",
    "                then find in which year did he play for Real Madird?\n",
    "                Once you find both answers, find how many years it has been since he left Real Madrid.'''\n",
    "\n",
    "# Observe how the agent navigates through LLM outputs using correct tool to arrive at the final answer.\n",
    "response = agent({\"input\":input_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4babd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
